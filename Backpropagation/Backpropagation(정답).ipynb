{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCLayer의 Backpropagation을 이용한 weight update실습\n",
    "\n",
    "다음과 같은 내용을 손글씨 내용정리를 통하여 알고 있습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "$Input : a^\\ell$ \\\n",
    "$z^\\ell = w^\\ell a^{\\ell-1} + b^\\ell$ and $a^\\ell = \\sigma(z^\\ell)$ &nbsp; where &nbsp; $\\ell = 2,3, ... L$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "output error : $\\delta^l = {\\partial L\\over\\partial z^\\ell} = {\\partial L\\over\\partial a^\\ell} {\\partial a^\\ell\\over\\partial z^\\ell}$ &nbsp; where &nbsp; $L$ : Loss \\\n",
    "$\\delta^\\ell = (w^\\ell)^Td^{\\ell+1} \\circledcirc \\sigma(z^\\ell)$ &nbsp; where &nbsp; $\\ell = (\\ell-1), (\\ell-2), ... 2$\n",
    "* 이때 실습에서는 MSELoss를 사용했습니다. \\\n",
    "MSELoss = $\\frac{1}{N} \\sum_{i=1}^{N}(a^L_i-t_i)^2$ &nbsp; where &nbsp; $t = target$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight update\n",
    "$w^\\ell$ -> $w^\\ell - lr {\\partial L\\over\\partial w^\\ell}$ &nbsp; where &nbsp; ${\\partial L\\over\\partial w^\\ell} = \\delta^\\ell (a^{\\ell-1})^T$ \\\n",
    "$b^\\ell$ -> $b^\\ell - lr {\\partial L\\over\\partial b^\\ell}$ &nbsp; where &nbsp; ${\\partial L\\over\\partial b^\\ell} = \\delta^\\ell$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (문제) numpy를 이용한 NumpyNet을 구현 하세요\n",
    "- 위 내용을 참고하여 아래의 빈칸을 채워 넣으세요\n",
    "- 아래 파이토치 모델과 동일하게 weight update하는 모델을 만드는 것이 목표입니다.\n",
    "- 아래의 예제문제를 통하여 weight를 확인해 보세요!\n",
    "\n",
    "예제 1 : 2layer model입니다. weight, bias로 이루어져 있습니다. \\\n",
    "예제 2 : 2layer model입니다. weight, bias, activation로 이루어져 있습니다.  \\\n",
    "예제 3 : 3layer model입니다. Batch학습이 가능하도록 설계해봅니다. \\\n",
    "예제 4 : 3layer model입니다. Batch학습과 output이 2차원 일때 테스트 해봅니다!\n",
    "- 자세한 내용은 각 문제를 확인해주세요!\n",
    "- 아래의 정답 유무에서는 pytorch 모델과 근사치를 구하는 것이 다를 수 있으므로 오차를 $10^{-6}$ 범위내에서는 같다고 판단하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NumpyNet:\n",
    "    def __init__(self, Weight, Bias, Activation=lambda x: x):\n",
    "        assert len(Weight) == len(Bias), \"prdict와 target의 길이가 같아야합니다.\"\n",
    "        self.weight = [np.array(w, dtype=np.float64) for w in Weight]\n",
    "        self.bias = [np.array(b, dtype=np.float64) for b in Bias]\n",
    "        self.f = Activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(type(x), np.ndarray):\n",
    "            x = np.array(x, dtype=np.float64)\n",
    "\n",
    "        self.forward_result = [x]\n",
    "        for w, b in zip(self.weight, self.bias):\n",
    "            z = w @ x + b\n",
    "            x = self.f(z)\n",
    "            self.forward_result.append(z)\n",
    "        return x.squeeze()\n",
    "\n",
    "    def backward(self, predict, target, h=1e-8):\n",
    "        assert len(predict) == len(target), \"prdict와 target의 길이가 같아야합니다.\"\n",
    "\n",
    "        loss = self.mse_loss(predict, target)\n",
    "        for idx in range(len(self.weight) - 1, 0, -1):\n",
    "            w = self.weight[idx]\n",
    "            d = self.backward_result[0]\n",
    "            z = self.forward_result[idx]\n",
    "            new_d = (w.T @ d) * self.diff(self.f, z, h)\n",
    "            self.backward_result.insert(0, new_d)\n",
    "        return loss\n",
    "\n",
    "    def weight_update(self, lr=0.01):\n",
    "        if len(self.forward_result[0].shape) == 1:\n",
    "            fr = [np.expand_dims(f, 0) for f in self.forward_result[:-1]]\n",
    "            br = [np.expand_dims(b, 1) for b in self.backward_result]\n",
    "            self.weight = [w - lr * (b @ self.f(z)) for w, z, b in zip(self.weight, fr, br)]\n",
    "            self.bias = [bia - lr * b.squeeze() for bia, b in zip(self.bias, br)]\n",
    "        else:\n",
    "            fr = self.forward_result[:-1]\n",
    "            br = self.backward_result\n",
    "            self.weight = [w - lr * (b @ self.f(z.T)) for w, z, b in zip(self.weight, fr, br)]\n",
    "            self.bias = [bia - lr * b.sum(axis=1, keepdims=True) for bia, b in zip(self.bias, br)]\n",
    "\n",
    "    def mse_loss(self, predict, target, h=1e-8):\n",
    "        if not isinstance(np.ndarray, type(target)):\n",
    "            target = np.array(target, dtype=np.float64)\n",
    "\n",
    "        eps = np.identity(len(target)) * h\n",
    "        if len(target.shape) == 1:\n",
    "            loss_rh = self.mse((predict + eps), target)\n",
    "            loss_lh = self.mse((predict - eps), target)\n",
    "            gap = np.diag(loss_rh - loss_lh)\n",
    "        else:\n",
    "            predict = np.expand_dims(predict.T, 1)\n",
    "            target = np.expand_dims(target.T, 1)\n",
    "            loss_rh = self.mse((predict + eps), target).squeeze()\n",
    "            loss_lh = self.mse((predict - eps), target).squeeze()\n",
    "            gap = np.diagonal(loss_rh - loss_lh, axis1=1, axis2=2).T\n",
    "\n",
    "        d = gap / (2 * h) * self.diff(self.f, self.forward_result[-1], h)\n",
    "        self.backward_result = [d]\n",
    "\n",
    "        loss = self.mse(predict, target)\n",
    "        return loss.sum()\n",
    "\n",
    "    def mse(self, predict, target):\n",
    "        result = np.divide(np.power(predict - target, 2), np.product(target.shape))\n",
    "        return result\n",
    "\n",
    "    def diff(self, f, x, h):\n",
    "        result = (f(x + h) - f(x - h)) / (2 * h)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch를 이용한 TorchNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "class TorchNet(nn.Module):\n",
    "    def __init__(self, Weight, Bias, Activation=False):\n",
    "        super().__init__()\n",
    "        self.net = nn.ModuleList()\n",
    "        for w, b in zip(Weight, Bias):\n",
    "            w = torch.tensor(w, dtype=torch.float64)\n",
    "            b = torch.tensor(b, dtype=torch.float64)\n",
    "            fc = nn.Linear(*torch.t(w).size())\n",
    "            fc.weight = nn.Parameter(w)\n",
    "            fc.bias = nn.Parameter(b.squeeze())\n",
    "            self.net.append(fc)\n",
    "            if Activation:\n",
    "                self.net.append(Activation())\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(torch.tensor, type(x)):\n",
    "            x = torch.tensor(x, dtype=torch.float64)\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제1\n",
    "- input, target, weight, bias를 표현하면 다음과 같습니다.\n",
    "\n",
    "$a^1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ &nbsp; and &nbsp;  $t = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}$ \\\n",
    "$w^2 = \\begin{pmatrix} 1 & -2 \\\\ 2 & 4 \\\\ -3 & 1 \\end{pmatrix}$ &nbsp; and &nbsp; $b^2 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$ \\\n",
    "$w^3 = \\begin{pmatrix} 1 & 2 & -3 \\\\ 2 & -1 & 3 \\end{pmatrix}$ &nbsp; and &nbsp; $b^3 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ \\\n",
    "$activation function : \\sigma(x) = x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, target = [1, 1], [2, 3]\n",
    "\n",
    "weight1 = [[1, -2], [2, 4], [-3, 1]]\n",
    "weight2 = [[1, 2, -3], [2, -1, 3]]\n",
    "weight = [weight1, weight2]\n",
    "\n",
    "bias1 = [1, 2, 3]\n",
    "bias2 = [2, 1]\n",
    "bias = [bias1, bias2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "넘파이 학습\n",
      "['1.0100', '-1.9900', '1.6700', '3.6700', '-2.4000', '1.6000']\n",
      "['1.0000', '0.9600', '-3.1300', '2.0000', '-0.4400', '3.0700']\n",
      "['1.0100', '1.6700', '3.6000']\n",
      "['1.8700', '1.0700']\n",
      "파이토치 학습\n",
      "['1.0100', '-1.9900', '1.6700', '3.6700', '-2.4000', '1.6000']\n",
      "['1.0000', '0.9600', '-3.1300', '2.0000', '-0.4400', '3.0700']\n",
      "['1.0100', '1.6700', '3.6000']\n",
      "['1.8700', '1.0700']\n",
      "\n",
      "\n",
      "정답 입니다!\n"
     ]
    }
   ],
   "source": [
    "print(\"넘파이 학습\")\n",
    "numpy_net = NumpyNet(Weight=weight, Bias=bias)\n",
    "for epoch in range(1):\n",
    "    outputs = numpy_net(input_data)\n",
    "    numpy_net.backward(outputs, target)\n",
    "    numpy_net.weight_update()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,numpy_net.weight[0]),[])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,numpy_net.weight[1]),[])])\n",
    "print([f\"{i:.4f}\" for i in numpy_net.bias[0]])\n",
    "print([f\"{i:.4f}\" for i in numpy_net.bias[1]])\n",
    "\n",
    "print(\"파이토치 학습\")\n",
    "torch_net = TorchNet(Weight=weight, Bias=bias)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(torch_net.parameters(), lr=0.01)\n",
    "for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = torch_net(input_data)\n",
    "    loss = criterion(outputs, torch.tensor(target, dtype=torch.float64))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,torch_net.net[0].weight),[])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,torch_net.net[1].weight),[])])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[0].bias])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[1].bias])\n",
    "\n",
    "print(\"\\n\")\n",
    "result1 = (numpy_net.weight[0] - torch_net.net[0].weight.detach().numpy() <= 1e-6).all()\n",
    "result2 = (numpy_net.weight[1] - torch_net.net[1].weight.detach().numpy() <= 1e-6).all()\n",
    "result3 = (numpy_net.bias[0] - torch_net.net[0].bias.detach().numpy() <= 1e-6).all()\n",
    "result4 = (numpy_net.bias[1] - torch_net.net[1].bias.detach().numpy() <= 1e-6).all()\n",
    "print(\"정답 입니다!\" if all([result1,result2,result3,result4]) else \"오답 입니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제 2\n",
    "- input, target, weight, bias를 표현하면 다음과 같습니다.\n",
    "- 예제 1과 다르게 activation이 추가되었습니다.\n",
    "\n",
    "$a^1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ &nbsp; and &nbsp;  $t = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}$ \\\n",
    "$w^2 = \\begin{pmatrix} 1 & -2 \\\\ 1 & 1 \\end{pmatrix}$ &nbsp; and &nbsp; $b^2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ \\\n",
    "$w^3 = \\begin{pmatrix} 1 & -1 \\\\ 2 & -1 \\end{pmatrix}$ &nbsp; and &nbsp; $b^3 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ \\\n",
    "$activation function : \\sigma(x) = x^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, target = [1, 1], [2, 3]\n",
    "\n",
    "weight1 = [[1, -2], [1, 1]]\n",
    "weight2 = [[1, -1], [2, -1]]\n",
    "weight = [weight1, weight2]\n",
    "\n",
    "bias1 = [0, 0]\n",
    "bias2 = [0, 0]\n",
    "bias = [bias1, bias2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "넘파이 학습\n",
      "['-0.0000', '-3.0000', '-0.8400', '-0.8400']\n",
      "['1.4200', '0.6800', '2.0400', '-0.8400']\n",
      "['-1.0000', '-1.8400']\n",
      "['0.4200', '0.0400']\n",
      "파이토치 학습\n",
      "['0.0000', '-3.0000', '-0.8400', '-0.8400']\n",
      "['1.4200', '0.6800', '2.0400', '-0.8400']\n",
      "['-1.0000', '-1.8400']\n",
      "['0.4200', '0.0400']\n",
      "\n",
      "\n",
      "정답 입니다!\n"
     ]
    }
   ],
   "source": [
    "print(\"넘파이 학습\")\n",
    "numpy_net = NumpyNet(Weight=weight, Bias=bias, Activation=lambda x: x**2)\n",
    "for epoch in range(1):\n",
    "    outputs = numpy_net(input_data)\n",
    "    numpy_net.backward(outputs, target)\n",
    "    numpy_net.weight_update()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,numpy_net.weight[0]),[])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,numpy_net.weight[1]),[])])\n",
    "print([f\"{i:.4f}\" for i in numpy_net.bias[0]])\n",
    "print([f\"{i:.4f}\" for i in numpy_net.bias[1]])\n",
    "\n",
    "class SqureActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x**2\n",
    "\n",
    "print(\"파이토치 학습\")\n",
    "active = SqureActivation\n",
    "torch_net = TorchNet(weight, bias, active)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(torch_net.parameters(), lr=0.01)\n",
    "for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = torch_net(input_data)\n",
    "    loss = criterion(outputs, torch.tensor(target, dtype=torch.float64))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,torch_net.net[0].weight),[])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,torch_net.net[2].weight),[])])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[0].bias])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[2].bias])\n",
    "\n",
    "print(\"\\n\")\n",
    "result1 = (numpy_net.weight[0] - torch_net.net[0].weight.detach().numpy() <= 1e-6).all()\n",
    "result2 = (numpy_net.weight[1] - torch_net.net[2].weight.detach().numpy() <= 1e-6).all()\n",
    "result3 = (numpy_net.bias[0] - torch_net.net[0].bias.detach().numpy() <= 1e-6).all()\n",
    "result4 = (numpy_net.bias[1] - torch_net.net[2].bias.detach().numpy() <= 1e-6).all()\n",
    "print(\"정답 입니다!\" if all([result1,result2,result3,result4]) else \"오답 입니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제 3\n",
    "- Batch 학습이 가능하도록 설계해보세요!\n",
    "- input data와 target은 다음과 같이 한 쌍입니다. $(a^1_i,t_i)$ where $i = 1, 2, 3$\n",
    "- input data 행렬에 대하여 pytorch는 행을 기준으로 서로 다른 데이터임을 구분하지만 주어진 input data는 열을 기준으로 하고 있습니다.\n",
    "\n",
    "$a^1_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, &nbsp; $a^1_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, &nbsp; $a^1_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, &nbsp; -> &nbsp; $a^1 = \\begin{pmatrix} 1 & 1 & 1 \\\\ 2 & -1 & 1 \\end{pmatrix}$ \n",
    "\n",
    "$t_1 = \\begin{pmatrix} -1 \\end{pmatrix}$, &nbsp; $t_2 = \\begin{pmatrix} 0 \\end{pmatrix}$, &nbsp; $t_3 = \\begin{pmatrix} 1 \\end{pmatrix}$, &nbsp; -> &nbsp; $t = \\begin{pmatrix} -1 & 0 & 1 \\end{pmatrix}$ \n",
    "\n",
    "$w^2 = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\\\ 2 & 3 \\end{pmatrix}$ &nbsp; and &nbsp; $b^2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$ \n",
    "\n",
    "$w^3 = \\begin{pmatrix} 2 & 3 & 1 \\\\ 2 & -1 & 2 \\end{pmatrix}$ &nbsp; and &nbsp; $b^3 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ \n",
    "\n",
    "$w^4 = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$ &nbsp; and &nbsp; $b^4 = \\begin{pmatrix} 0 \\end{pmatrix}$ \n",
    "\n",
    "$activation function : \\sigma(x) = x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [[1, 1, 1], [2, -1, 1]]\n",
    "target = [-1, 0, 1]\n",
    "\n",
    "weight1 = [[1, 2], [2, 1], [-1, 2]]\n",
    "weight2 = [[2, 3, 1], [2, -1, 2]]\n",
    "weight3 = [[1, 1]]\n",
    "weight = [weight1, weight2, weight3]\n",
    "\n",
    "bias1 = [[0], [0], [0]]\n",
    "bias2 = [[0], [0]]\n",
    "bias3 = [[0]]\n",
    "bias = [bias1, bias2, bias3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "넘파이 학습\n",
      "['-0.2533', '-0.8533', '1.3733', '-0.4267', '-1.9400', '-0.1400']\n",
      "['0.2600', '1.6600', '-0.1133', '0.2600', '-2.3400', '0.8867']\n",
      "['-7.6133', '-3.3667']\n",
      "['-1.2533', '-0.6267', '-0.9400']\n",
      "['-0.3133', '-0.3133']\n",
      "['-0.3133']\n",
      "파이토치 학습\n",
      "['-0.2533', '-0.8533', '1.3733', '-0.4267', '-1.9400', '-0.1400']\n",
      "['0.2600', '1.6600', '-0.1133', '0.2600', '-2.3400', '0.8867']\n",
      "['-7.6133', '-3.3667']\n",
      "['-1.2533', '-0.6267', '-0.9400']\n",
      "['-0.3133', '-0.3133']\n",
      "['-0.3133']\n",
      "\n",
      "\n",
      "정답 입니다!\n"
     ]
    }
   ],
   "source": [
    "print(\"넘파이 학습\")\n",
    "numpy_net = NumpyNet(weight, bias)\n",
    "for epoch in range(1):\n",
    "    outputs = numpy_net(input_data)\n",
    "    numpy_net.backward(outputs, target)\n",
    "    numpy_net.weight_update()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.weight[0]), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.weight[1]), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.weight[2]), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.bias[0]), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.bias[1]), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.bias[2]), [])])\n",
    "\n",
    "print(\"파이토치 학습\")\n",
    "torch_net = TorchNet(weight, bias)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(torch_net.parameters(), lr=0.01)\n",
    "for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = torch_net(np.array(input_data).T)\n",
    "    loss = criterion(outputs, torch.tensor([target], dtype=torch.float64).T)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, torch_net.net[0].weight), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, torch_net.net[1].weight), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, torch_net.net[2].weight), [])])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[0].bias])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[1].bias])\n",
    "print([f\"{float(torch_net.net[2].bias):.4f}\"])\n",
    "\n",
    "print(\"\\n\")\n",
    "result1 = (numpy_net.weight[0] - torch_net.net[0].weight.detach().numpy() <= 1e-6).all()\n",
    "result2 = (numpy_net.weight[1] - torch_net.net[1].weight.detach().numpy() <= 1e-6).all()\n",
    "result3 = (numpy_net.weight[2] - torch_net.net[2].weight.detach().numpy() <= 1e-6).all()\n",
    "result4 = (numpy_net.bias[0].squeeze() - torch_net.net[0].bias.detach().numpy() <= 1e-6).all()\n",
    "result5 = (numpy_net.bias[1].squeeze() - torch_net.net[1].bias.detach().numpy() <= 1e-6).all()\n",
    "result6 = (numpy_net.bias[2].squeeze() - torch_net.net[2].bias.detach().numpy() <= 1e-6).all()\n",
    "print(\"정답 입니다!\" if all([result1,result2,result3,result4,result5,result6]) else \"오답 입니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제 4\n",
    "\n",
    "- input data와 target은 다음과 같이 한 쌍입니다. $(a^1_i,t_i)$ where $i = 1, 2, 3$\n",
    "\n",
    "$a^1_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, &nbsp; $a^1_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, &nbsp; $a^1_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, &nbsp; -> &nbsp; $a^1 = \\begin{pmatrix} 1 & 1 & 1 \\\\ 2 & -1 & 1 \\end{pmatrix}$ \n",
    "\n",
    "$t_1 = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$, &nbsp; $t_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, &nbsp; $t_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, &nbsp; -> &nbsp; $t = \\begin{pmatrix} -1 & 0 & 1 \\\\ 2 & 1 & 1 \\end{pmatrix}$ \n",
    "\n",
    "$w^2 = \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\\\ -1 & 2 \\end{pmatrix}$ &nbsp; and &nbsp; $b^2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$ \n",
    "\n",
    "$w^3 = \\begin{pmatrix} 2 & 3 & 1 \\\\ 2 & -1 & 2 \\end{pmatrix}$ &nbsp; and &nbsp; $b^3 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ \n",
    "\n",
    "$w^4 = \\begin{pmatrix} 1 & -1 \\\\ 2 & 1 \\end{pmatrix}$ &nbsp; and &nbsp; $b^4 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ \n",
    "\n",
    "$activation function : \\sigma(x) = x$\n",
    "\n",
    "- output이 2차원 데이터 이므로 Loss를 다음과 같이 계산해야 합니다.\n",
    "- MSELoss = $\\frac{1}{N*M} \\sum_{i=1}^{N}\\sum_{j=1}^{M}(a^L_{i,j}-t_{i,j})^2$ &nbsp; where &nbsp; $t = target$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [[1, 1, 1], [2, -1, 1]]\n",
    "target = [[-1, 0, 1], [2, 1, 1]]\n",
    "\n",
    "weight1 = [[1, 2], [2, 1], [-1, 2]]\n",
    "weight2 = [[2, 3, 1], [2, -1, 2]]\n",
    "weight3 = [[1, -1], [2, 1]]\n",
    "weight = [weight1, weight2, weight3]\n",
    "\n",
    "bias1 = [[0], [0], [0]]\n",
    "bias2 = [[0], [0]]\n",
    "bias3 = [[0], [0]]\n",
    "bias = [bias1, bias2, bias3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "넘파이 학습\n",
      "['-0.6400', '-1.4000', '0.2200', '-2.2467', '-1.9900', '-0.1633']\n",
      "['-1.1233', '0.4633', '-0.8233', '0.9033', '-1.8033', '1.2433']\n",
      "['-0.6533', '-1.5167', '-5.0133', '-2.4200']\n",
      "['-1.6400', '-1.7800', '-0.9900']\n",
      "['-0.6500', '-0.1700']\n",
      "['-0.1033', '-0.2733']\n",
      "파이토치 학습\n",
      "['-0.6400', '-1.4000', '0.2200', '-2.2467', '-1.9900', '-0.1633']\n",
      "['-1.1233', '0.4633', '-0.8233', '0.9033', '-1.8033', '1.2433']\n",
      "['-0.6533', '-1.5167', '-5.0133', '-2.4200']\n",
      "['-1.6400', '-1.7800', '-0.9900']\n",
      "['-0.6500', '-0.1700']\n",
      "['-0.1033', '-0.2733']\n",
      "\n",
      "\n",
      "정답 입니다!\n"
     ]
    }
   ],
   "source": [
    "print(\"넘파이 학습\")\n",
    "numpy_net = NumpyNet(weight, bias)\n",
    "for epoch in range(1):\n",
    "    outputs = numpy_net(input_data)\n",
    "    numpy_net.backward(outputs, target)\n",
    "    numpy_net.weight_update()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.weight[0]), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.weight[1]), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.weight[2]), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.bias[0]), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.bias[1]), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, numpy_net.bias[2]), [])])\n",
    "\n",
    "print(\"파이토치 학습\")\n",
    "torch_net = TorchNet(weight, bias)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(torch_net.parameters(), lr=0.01)\n",
    "for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = torch_net(np.array(input_data).T)\n",
    "    loss = criterion(outputs, torch.tensor(target, dtype=torch.float64).T)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, torch_net.net[0].weight), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, torch_net.net[1].weight), [])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list, torch_net.net[2].weight), [])])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[0].bias])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[1].bias])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[2].bias])\n",
    "\n",
    "print(\"\\n\")\n",
    "result1 = (numpy_net.weight[0] - torch_net.net[0].weight.detach().numpy() <= 1e-6).all()\n",
    "result2 = (numpy_net.weight[1] - torch_net.net[1].weight.detach().numpy() <= 1e-6).all()\n",
    "result3 = (numpy_net.weight[2] - torch_net.net[2].weight.detach().numpy() <= 1e-6).all()\n",
    "result4 = (numpy_net.bias[0].squeeze() - torch_net.net[0].bias.detach().numpy() <= 1e-6).all()\n",
    "result5 = (numpy_net.bias[1].squeeze() - torch_net.net[1].bias.detach().numpy() <= 1e-6).all()\n",
    "result6 = (numpy_net.bias[2].squeeze() - torch_net.net[2].bias.detach().numpy() <= 1e-6).all()\n",
    "print(\"정답 입니다!\" if all([result1,result2,result3,result4,result5,result6]) else \"오답 입니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version\n",
    "\n",
    "- Python==3.8.18\n",
    "- torch==1.13.1\n",
    "- numpy==1.24.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

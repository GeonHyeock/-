{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCLayer의 Backpropagation을 이용한 weight update실습\n",
    "\n",
    "다음과 같은 내용을 손글씨 내용정리를 통하여 알고 있습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "$Input : a^\\ell$ \\\n",
    "$z^\\ell = w^\\ell a^{\\ell-1} + b^\\ell$ and $a^\\ell = \\sigma(z^\\ell)$ &nbsp; where &nbsp; $\\ell = 2,3, ... L$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "output error : $\\delta^l = {\\partial L\\over\\partial z^\\ell} = {\\partial L\\over\\partial a^\\ell} {\\partial a^\\ell\\over\\partial z^\\ell}$ &nbsp; where &nbsp; $L$ : Loss \\\n",
    "$\\delta^\\ell = (w^\\ell)^Td^{\\ell+1} \\circledcirc \\sigma(z^\\ell)$ &nbsp; where &nbsp; $\\ell = (\\ell-1), (\\ell-2), ... 2$\n",
    "* 이때 실습에서는 MSELoss를 사용했습니다. \\\n",
    "MSELoss = $\\frac{1}{N} \\sum_{i=1}^{N}(a^L_i-t_i)^2$ &nbsp; where &nbsp; $t = target$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight update\n",
    "$w^\\ell$ -> $w^\\ell - lr {\\partial L\\over\\partial w^\\ell}$ &nbsp; where &nbsp; ${\\partial L\\over\\partial w^\\ell} = \\delta^\\ell (a^{\\ell-1})^T$ \\\n",
    "$b^\\ell$ -> $b^\\ell - lr {\\partial L\\over\\partial b^\\ell}$ &nbsp; where &nbsp; ${\\partial L\\over\\partial b^\\ell} = \\delta^\\ell$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (문제) numpy를 이용한 NumpyNet을 구현 하세요\n",
    "- 위 내용을 참고하여 아래의 빈칸을 채워 넣으세요\n",
    "- 아래 파이토치 모델과 동일한 weight update모델을 만드는 것이 목표입니다.\n",
    "- 아래의 예제문제를 통하여 weight의 변화를 확인하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NumpyNet:\n",
    "    def __init__(self, Weight, Bias, Activation=lambda x: x):\n",
    "        assert len(Weight) == len(Bias), \"prdict와 target의 길이가 같아야합니다.\"\n",
    "        self.weight = [np.array(w, dtype=np.float64) for w in Weight]\n",
    "        self.bias = [np.array(b, dtype=np.float64) for b in Bias]\n",
    "        self.f = Activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(type(x), np.ndarray):\n",
    "            x = np.array(x, dtype=np.float64)\n",
    "\n",
    "        self.forward_result = [x]\n",
    "        for w, b in zip(self.weight, self.bias):\n",
    "            z = w @ x + b\n",
    "            x = self.f(z)\n",
    "            self.forward_result.append(z)\n",
    "        return x\n",
    "\n",
    "    def backward(self, predict, target, h=1e-8):\n",
    "        assert len(predict) == len(target), \"prdict와 target의 길이가 같아야합니다.\"\n",
    "        \n",
    "        loss = self.mse_loss(predict, target)\n",
    "        for idx in range(len(self.weight) - 1, 0, -1):\n",
    "            w = self.weight[idx]\n",
    "            d = self.backward_result[-1]\n",
    "            z = self.forward_result[idx]\n",
    "            new_d = (w.T @ d) * self.diff(self.f, z, h)\n",
    "            self.backward_result.insert(0, new_d)\n",
    "        return loss\n",
    "\n",
    "    def weight_update(self, lr=0.01):\n",
    "        fr = [np.expand_dims(f, 0) for f in self.forward_result[:-1]]\n",
    "        br = [np.expand_dims(b, 1) for b in self.backward_result]\n",
    "\n",
    "        self.weight = [w - lr * (b @ self.f(z)) for w, z, b in zip(self.weight, fr, br)]\n",
    "        self.bias = [bia - lr * b.squeeze() for bia, b in zip(self.bias, br)]\n",
    "\n",
    "    def mse_loss(self, predict, target, h=1e-8):\n",
    "        if not isinstance(np.ndarray, type(target)):\n",
    "            target = np.array(target, dtype=np.float64)\n",
    "\n",
    "        eps = np.identity(len(predict)) * h\n",
    "        loss_rh, loss_lh = self.mse((predict + eps), target), self.mse((predict - eps), target)\n",
    "        d = np.diag((loss_rh - loss_lh) / (2 * h)) * self.diff(self.f, self.forward_result[-1], h)\n",
    "        self.backward_result = [d]\n",
    "\n",
    "        loss = self.mse(predict, target)\n",
    "        return loss.sum()\n",
    "\n",
    "    def mse(self, predict, target):\n",
    "        return np.divide(np.power(predict - target, 2), len(predict))\n",
    "\n",
    "    def diff(self, f, x, h):\n",
    "        return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch를 이용한 TorchNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TorchNet(nn.Module):\n",
    "    def __init__(self, Weight, Bias, Activation=False):\n",
    "        super().__init__()\n",
    "        self.net = nn.ModuleList()\n",
    "        for w, b in zip(Weight, Bias):\n",
    "            w = torch.tensor(w, dtype=torch.float64)\n",
    "            b = torch.tensor(b, dtype=torch.float64)\n",
    "            fc = nn.Linear(*torch.t(w).size())\n",
    "            fc.weight = nn.Parameter(w)\n",
    "            fc.bias = nn.Parameter(b)\n",
    "            self.net.append(fc)\n",
    "            if Activation:\n",
    "                self.net.append(Activation())\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(torch.tensor, type(x)):\n",
    "            x = torch.tensor(x, dtype=torch.float64)\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제1\n",
    "- input, target, weight, bias를 표현하면 다음과 같습니다.\n",
    "\n",
    "$a^1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ &nbsp; and &nbsp;  $t = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}$ \\\n",
    "$w^2 = \\begin{pmatrix} 1 & -2 \\\\ 2 & 4 \\\\ -3 & 1 \\end{pmatrix}$ &nbsp; and &nbsp; $b^2 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$ \\\n",
    "$w^3 = \\begin{pmatrix} 1 & 2 & -3 \\\\ 2 & -1 & 3 \\end{pmatrix}$ &nbsp; and &nbsp; $b^3 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ \\\n",
    "$activation function : \\sigma(x) = x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, target = [1, 1], [2, 3]\n",
    "\n",
    "weight1 = [[1, -2], [2, 4], [-3, 1]]\n",
    "weight2 = [[1, 2, -3], [2, -1, 3]]\n",
    "weight = [weight1, weight2]\n",
    "\n",
    "bias1 = [1, 2, 3]\n",
    "bias2 = [2, 1]\n",
    "bias = [bias1, bias2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 넘파이 학습 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.0100', '-1.9900', '1.6700', '3.6700', '-2.4000', '1.6000']\n",
      "['1.0000', '0.9600', '-3.1300', '2.0000', '-0.4400', '3.0700']\n",
      "['1.0100', '1.6700', '3.6000']\n",
      "['1.8700', '1.0700']\n"
     ]
    }
   ],
   "source": [
    "numpy_net = NumpyNet(Weight=weight, Bias=bias)\n",
    "for epoch in range(1):\n",
    "    outputs = numpy_net(input_data)\n",
    "    numpy_net.backward(outputs, target)\n",
    "    numpy_net.weight_update()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,numpy_net.weight[0]),[])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,numpy_net.weight[1]),[])])\n",
    "print([f\"{i:.4f}\" for i in numpy_net.bias[0]])\n",
    "print([f\"{i:.4f}\" for i in numpy_net.bias[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치 학습결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.0100', '-1.9900', '1.6700', '3.6700', '-2.4000', '1.6000']\n",
      "['1.0000', '0.9600', '-3.1300', '2.0000', '-0.4400', '3.0700']\n",
      "['1.0100', '1.6700', '3.6000']\n",
      "['1.8700', '1.0700']\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "torch_net = TorchNet(Weight=weight, Bias=bias)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(torch_net.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = torch_net(input_data)\n",
    "    loss = criterion(outputs, torch.tensor(target, dtype=torch.float64))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,torch_net.net[0].weight),[])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,torch_net.net[1].weight),[])])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[0].bias])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[1].bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제 2\n",
    "- input, target, weight, bias를 표현하면 다음과 같습니다.\n",
    "- 예제 1과 다르게 activation이 추가되었습니다.\n",
    "\n",
    "$a^1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ &nbsp; and &nbsp;  $t = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}$ \\\n",
    "$w^2 = \\begin{pmatrix} 1 & -2 \\\\ 1 & 1 \\end{pmatrix}$ &nbsp; and &nbsp; $b^2 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ \\\n",
    "$w^3 = \\begin{pmatrix} 1 & -1 \\\\ 2 & -1 \\end{pmatrix}$ &nbsp; and &nbsp; $b^3 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ \\\n",
    "$activation function : \\sigma(x) = x^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, target = [1, 1], [2, 3]\n",
    "\n",
    "weight1 = [[1, -2], [1, 1]]\n",
    "weight2 = [[1, -1], [2, -1]]\n",
    "weight = [weight1, weight2]\n",
    "\n",
    "bias1 = [0, 0]\n",
    "bias2 = [0, 0]\n",
    "bias = [bias1, bias2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 넘파이 학습 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.0000', '-3.0000', '-0.8400', '-0.8400']\n",
      "['1.4200', '0.6800', '2.0400', '-0.8400']\n",
      "['-1.0000', '-1.8400']\n",
      "['0.4200', '0.0400']\n"
     ]
    }
   ],
   "source": [
    "numpy_net = NumpyNet(Weight=weight, Bias=bias, Activation=lambda x: x**2)\n",
    "for epoch in range(1):\n",
    "    outputs = numpy_net(input_data)\n",
    "    numpy_net.backward(outputs, target)\n",
    "    numpy_net.weight_update()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,numpy_net.weight[0]),[])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,numpy_net.weight[1]),[])])\n",
    "print([f\"{i:.4f}\" for i in numpy_net.bias[0]])\n",
    "print([f\"{i:.4f}\" for i in numpy_net.bias[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치 학습결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.0000', '-3.0000', '-0.8400', '-0.8400']\n",
      "['1.4200', '0.6800', '2.0400', '-0.8400']\n",
      "['-1.0000', '-1.8400']\n",
      "['0.4200', '0.0400']\n"
     ]
    }
   ],
   "source": [
    "class SqureActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x**2\n",
    "\n",
    "\n",
    "active = SqureActivation\n",
    "torch_net = TorchNet(weight, bias, active)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(torch_net.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = torch_net(input_data)\n",
    "    loss = criterion(outputs, torch.tensor(target, dtype=torch.float64))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,torch_net.net[0].weight),[])])\n",
    "print([f\"{i:.4f}\" for i in sum(map(list,torch_net.net[2].weight),[])])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[0].bias])\n",
    "print([f\"{i:.4f}\" for i in torch_net.net[2].bias])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
